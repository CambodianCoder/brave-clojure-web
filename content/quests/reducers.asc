---
title: "Reducers"
---

= (This Article is Barely About) Reducers

As a mild-mannered, somewhat timid programmer, I'm not a huge fan of
fastness in the physical world. "Slow down, ya tootin' maniac!" is
what I think to myself whenever I see 

* someone walking on an escalator
* someone 

In my role as a programmer, though, I wouldn't get very far with this
attitude. Luckily for me, Clojure's sweet little +core.reducers+
library is able to give programs a speed boost.

core.reducers is *TODO give a basic explanation*. 

+core.reducers+ introduces speed improvements by doing two
things. First, it lets you process collections in parallel (also known
as _data parallelism_) using Java's fork/join framework. Second, it
eliminates the creation of intermediate collections. Both of these _
were made possible by a re-thinking of what it means to process a
collection.

In this article, I'm going to focus on the data parallelism aspect of
reducers. In fact, I'm not even going to talk about reducers until
we're about 80% of the  way through the article, because I want you to
have a complete understanding of the context that they live in (and
because the ideas involved are super interesting.) It's basically a
primer on parallel programming, the same way the *concurrency chapter*
was a primer on concurrent programming.

*TODO Mention it's an introduction to performance*
*TODO Mention it's mostly concerned with no shared state*

So, to give you a complete understanding of the subject, I'm
going to start by briefly going over the basics of what concurrency
and parallelism are and why they matter. Next, I'll describe what
programmers actually mean by "performance", and touch on the three
main strategies available to improve it.

One of those strategies is data parallelism, and I'll spend a lot of
time on this topic. This is where things really start to heat up! In
this section, you'll learn about the *work-span model*, one of the
theoretical models used to reason about parallel performance. It's not
all theory, though; you'll also learn about the practical approaches
to writing parallel programs. I'll discuss how you can achieve the
balance between _minimizing overhead_ and _load balancing_ using
_thread management_, _granularity_, _parallel slack_, _tiling_, and
_fusion_. **TODO mention that java is involved**

All of this fun prep work will have you primed to understand the
*fork/join* framework. You'll learn how fork/join implements many of
the techniques mentioned above and adds a couple more to the mix,
including _recursive decomposition_ and _work stealing_.

And then we'll be ready to talk about the reducers library. You'll
look at some examples of how to use the reducers library so that you
parallelize performance, and you'll peek at some of the actual
implementation.

*MENTION that ideally, performance improves with hardware
improvements*

== Basics

TODO

=== The Three Performance Walls

The reason we need to care about concurrent and parallel programming
techniques is that computer hardware manufacturers have run into three
fundamental limitations, imposed by physics, that won't be overcome
any time soon &mdash; if ever. Because of these limitations, we can no
longer ... The limitations are known as:

* The Power Wall
* The Memory Wall
* The Instruction-Level Parallelism Wall

_The Power Wall_ is a limitation on CPU clock speeds. You've probably
noticed that clock speeds have barely inched forward over the last
decade, compared to the rapid progress of previous decades where clock
speeds followed Moore's law and doubled every eighteen months. The
reason for this near halt in progress is that chip designs have
reached a point where increasing clock speed results in exponential
increases in power consumption and heat, and no one wants to buy a
computer that costs as much to run as a UNIVAC.

Even if clock speeds _could_ be increased, the hardware would still
have to contend with the _Memory Wall_, which is the extreme disparity
between memory access time and CPU performance &mdash; CPUs can
process instructions much faster than they can fetch them from main
memory. Increasing clock speed would be like *TODO analogy*.


*TODO mention this is why we need explicit parallel techniques*
*TODO that the code we write is often serial even though it can
be considered parallel*

The final limitation, the _Instruction-Level Parallelism (ILP) Wall_
is a limitation on the level of parallelism that can be extracted from
serial (non-parallel) instructions. Much of the hullabaloo around
parallelism has focused on the fact that we're stuffing more cores
into CPU's, but in fact, even old-timey single-core machines have
parallel aspects to their architectures and are capable of running
serial instructions in parallel, to an extent. In fact, hardware can
automatically parallelize serial instructions to an extent. 

In an ideal world, hardware would be smart enough to automatically
parallelize everything that can be parallelized, but the fact is they
can't, and it looks like there won't be any significant improvements
any time soon.

Because of these three limitations, chip manufacturers have focused on
developing multi-core process instead of increasing clock speed. In
order to get the most performance out of these processors, we have to
structure our applications differently.

=== Concurrent and Parallel Programming

*TODO explain the "task" abstraction*

Concurrent and Parallel programming refer to the tools and techniques
you use to program for multiple processors. _Concurrency_ refers to a
system's ability to _manage_ more than one task at a time, while
_parallelism_ refers to a system's ability to _execute_ more than one
task at a time. From this perspective, parallelism is a sub-category
of concurrency.

Programmers usually the term _concurrency_ when referring to multiple,
independent tasks with access to shared state. For example, *TODO
example*. _Parallelism_ usually refers to decomposing a collection of
data into smaller chunks, processing those, and reassembling the
results. In this situation, there's no logical need for shared access
to state. Of course, you have to keep track of all of the different
computations.

*TODO talk about threading and scheduling*

==  Performance

So far, I've been talking about performance without defining it,
relying on the shared general sense of performance as the thing we
want to improve to the point that users don't say "This is slow and I
hate it." In this section, I'll break down performance, defining its
most relevant aspects. I'll also describe the high-level strategies we
use to improve it.

=== Performance Aspects

_Latency_ is the amount of time it takes to complete a task, and is
what we usually care about most because it has the most direct impact
on user experience. One example is _network latency_, or the amount of
time it takes for a packet to reach its destination. If you're
measuring the amount of time it takes to open a file or execute a
function or generate a report, those are all latency.

You can measure latency at any level of granularity. For example, if
you make web sites you've probably measured the total time it takes to
load a web page to decide if it needs optimization. At first, you
only care about the "load the page" task as a whole. If you discover
it's too slow, then you can drill down to individual network requests
to see what's causing problems. Drilling down further, you might find
that your SQL queries are taking a long time because your tables
aren't indexed properly, or something like that.

Most of this article focuses on how to effectively reduce latency
with parallel programming.

_Throughput_ is the number of tasks per second that your system can
perform. Your web server, for example, might be able to complete 1,000
requests per second.

*TODO EXPAND*
There's a direct relationship between latency and throughput. Let's
say you're running the world's lousiest web server, and it can only
handle one request per second. If a thousand people make a request to
this server at the same time, then on average it will take 500 seconds
to respond to a request. 

_Utilization_ is the degree to which a resource is used. It has two
flavors, _capacity-based_ utilization and _time-based_ utilization. In
this article we only care about the _time-based_ flavor, which is a
measure of how busy a resource is over a given unit of
time. Specifically, we care about CPU utilization, which is the
percentage of time that your CPU is doing work divided by some unit of
time.

One of the challenges with parallel programming is figuring how to
make efficient use of resources by ensuring that we reduce unnecesary
CPU idle time. Later in the article, you'll learn about techniques
that help you do this, including the powerful Fork/Join framework.

=== General Performance Strategies

There are three concurrent/parallel programming general strategies you
can use to help improve performance: _latency hiding_, _functional
decomposition_, and _data parallelism_. Guess what's coming next!
That's right, I'm going to explain those things!

==== Latency Hiding

*TODO betterify definition*
_Latency hiding_ is a fancy term for something you do all the
time. You're hiding latency whenever you move a task that's in a
waiting state to the background and focus on something else. 
Examples abound, not just in programming but in real life.

If you use Clojure's +future+ function to kick off a task in a
separate thread so that the main task can continue unimpeded, you're
hiding latency. I've used +future+ on web servers to send an email
without increasing the overall response time for a user's request.

Latency hiding is often a cheap and easy way to get quick performance
gains. On the other hand, forgetting to employ it can lead to some
dire consequences, as this comic illustrates:

*TODO image*

==== Functional Decomposition

_Functional decomposition_ is the term for when a multicultural group
of teenagers combine their powers to summon an avatar of the earth to
fight pollution:

*TODO image*

_Cough_ uh, I mean, _functional decomposition_ is the practice of
running logically independent aspects of your program on different
threads, so that none of  


* Explain how it differs from laziness
* No intermediate collections
* Talk about performance first?
* TODO lookup where I got my definition of efficiency as how well it
  makes use of computing resources / soakt he cores

== References

* Systems Performance: Enterprise and the Cloud
* Structured Parallel Programming
* Ebook on queueing systems

