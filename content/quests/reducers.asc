---
title: "Reducers"
---

= (This Article is Barely About) Reducers

Have you ever done multiple things at the same time? Don't be silly,
of course you have. You've made a sandwich while guiltily watching
_The Real Housewives of Orange County_ or texted while driving or
fantasized about which yacht to buy with your vast book proceeds while
vacuuming. (I don't know where I got these examples. They just came to
my mind for no reason whatsoever.)

Point is, life is full of doing multiple things at once. Until
recently, though, we programmers haven't had to deal with this
unpleasant fact while programming. Alas, the halcyon days of purely
serial code are over. It's time to adapt to the new reality, a reality
where you have to know how to write code for multiple processors if
you want your programs to have acceptable performance.

In _Clojure for the Brave and True_ I wrote about the
http://bravecljure.com/concurrency[state-management difficulties] that
you can run into when doing concurrent programming, and
http://bravecljure.com/concurrenc/zombie-metaphysics/[how Clojure can
help you deal with them]. But that's only half the story. If you
really want to master the art of doing multiple things at once, you
need to know about parallelism.

And hey, guess what, it just so happens that this article is about
parallelism. In the pages (screens?  not-yet-scrolled-to portion?)
ahead, you'll learn about Clojure's +core.reducers+ library, a great
option for doing parallel computation. Whereas +clojure.core+ provides
only +pmap+ to parallelize mapping, I'll show you how to use reducers
to parallelize +take+, +filter+, and more.

If I only show you how to use reducers, though, I'll have failed you
as an author and a gentleman. The world of parallel programming is
fascinating, and I aim to take you on a thorough tour of it so you'll
understand how the reducers library fits into the broader computing
landscape; you'll understand not just _what_ the reducers library
does, but _why_ and _how_.

You'll start by learning all about parallel performance. You'll learn
more about why it matters, and some general performance
strategies. Next, you'll dig deep into _data parallism_, where you'll
learn about the _work-span model_, one of the theoretical models used
to reason about parallel performance. It's not all theory, though;
you'll also learn about the practical approaches to writing parallel
programs. I'll discuss how you can achieve the balance between
_minimizing overhead_ and _load balancing_ using _thread management_,
_granularity_, _parallel slack_, _tiling_, and _fusion_.  You'll learn
about the _executors_ in Java's +java.util.concurrent+ package, which
Clojure uses extensively.

All of this fun prep work will have you primed to understand the
_fork/join_ framework. You'll learn how fork/join implements many of
the techniques mentioned above and adds a couple more to the mix,
including _recursive decomposition_ and _work stealing_.

And then we'll be ready to talk about the reducers library. You'll
look at some examples of how to use the reducers library to
parallelize image manipulation tasks, and you'll peek at the Clojure
implementation.

That's  a lot to learn! Let's get started!

*MENTION that ideally, performance improves with hardware
improvements*

== Basics

TODO

=== The Three Performance Walls

The reason we need to care about concurrent and parallel programming
techniques is that computer hardware manufacturers have run into three
fundamental limitations, imposed by physics, that won't be overcome
any time soon &mdash; if ever. Because of these limitations, we can no
longer ... The limitations are known as:

* The Power Wall
* The Memory Wall
* The Instruction-Level Parallelism Wall

_The Power Wall_ is a limitation on CPU clock speeds. You've probably
noticed that clock speeds have barely inched forward over the last
decade, compared to the rapid progress of previous decades where clock
speeds followed Moore's law and doubled every eighteen months. The
reason for this near halt in progress is that chip designs have
reached a point where increasing clock speed results in exponential
increases in power consumption and heat, and no one wants to buy a
computer that costs as much to run as a UNIVAC.

Even if clock speeds _could_ be increased, the hardware would still
have to contend with the _Memory Wall_, which is the extreme disparity
between memory access time and CPU performance &mdash; CPUs can
process instructions much faster than they can fetch them from main
memory. Increasing clock speed would be like *TODO analogy*.


*TODO mention this is why we need explicit parallel techniques*
*TODO that the code we write is often serial even though it can
be considered parallel*

The final limitation, the _Instruction-Level Parallelism (ILP) Wall_
is a limitation on the level of parallelism that can be extracted from
serial (non-parallel) instructions. Much of the hullabaloo around
parallelism has focused on the fact that we're stuffing more cores
into CPU's, but in fact, even old-timey single-core machines have
parallel aspects to their architectures and are capable of running
serial instructions in parallel, to an extent. In fact, hardware can
automatically parallelize serial instructions to an extent. 

In an ideal world, hardware would be smart enough to automatically
parallelize everything that can be parallelized, but the fact is they
can't, and it looks like there won't be any significant improvements
any time soon.

Because of these three limitations, chip manufacturers have focused on
developing multi-core process instead of increasing clock speed. In
order to get the most performance out of these processors, we have to
structure our applications differently.

=== Concurrent and Parallel Programming

*TODO explain the "task" abstraction*

Concurrent and Parallel programming refer to the tools and techniques
you use to program for multiple processors. _Concurrency_ refers to a
system's ability to _manage_ more than one task at a time, while
_parallelism_ refers to a system's ability to _execute_ more than one
task at a time. From this perspective, parallelism is a sub-category
of concurrency.

Programmers usually the term _concurrency_ when referring to multiple,
independent tasks with access to shared state. For example, *TODO
example*. _Parallelism_ usually refers to decomposing a collection of
data into smaller chunks, processing those, and reassembling the
results. In this situation, there's no logical need for shared access
to state. Of course, you have to keep track of all of the different
computations.

*TODO talk about threading and scheduling*

==  Performance

So far, I've been talking about performance without defining it,
relying on the shared general sense of performance as the thing we
want to improve to the point that users don't say "This is slow and I
hate it." In this section, I'll break down performance, defining its
most relevant aspects. I'll also describe the high-level strategies we
use to improve it.

=== Performance Aspects

_Latency_ is the amount of time it takes to complete a task, and is
what we usually care about most because it has the most direct impact
on user experience. One example is _network latency_, or the amount of
time it takes for a packet to reach its destination. If you're
measuring the amount of time it takes to open a file or execute a
function or generate a report, those are all latency.

You can measure latency at any level of granularity. For example, if
you make web sites you've probably measured the total time it takes to
load a web page to decide if it needs optimization. At first, you
only care about the "load the page" task as a whole. If you discover
it's too slow, then you can drill down to individual network requests
to see what's causing problems. Drilling down further, you might find
that your SQL queries are taking a long time because your tables
aren't indexed properly, or something like that.

Most of this article focuses on how to effectively reduce latency
with parallel programming.

_Throughput_ is the number of tasks per second that your system can
perform. Your web server, for example, might be able to complete 1,000
requests per second.

*TODO EXPAND*
There's a direct relationship between latency and throughput. Let's
say you're running the world's lousiest web server, and it can only
handle one request per second. If a thousand people make a request to
this server at the same time, then on average it will take 500 seconds
to respond to a request. 

_Utilization_ is the degree to which a resource is used. It has two
flavors, _capacity-based_ utilization and _time-based_ utilization. In
this article we only care about the _time-based_ flavor, which is a
measure of how busy a resource is over a given unit of
time. Specifically, we care about CPU utilization, which is the
percentage of time that your CPU is doing work divided by some unit of
time.

One of the challenges with parallel programming is figuring how to
make efficient use of resources by ensuring that we reduce unnecesary
CPU idle time. Later in the article, you'll learn about techniques
that help you do this, including the powerful Fork/Join framework.

*TODO Speedup*

=== General Performance Strategies

There are three concurrent/parallel programming general strategies you
can use to help improve performance: _latency hiding_, _functional
decomposition_, and _data parallelism_. Guess what's coming next!
That's right, I'm going to explain those things!

==== Latency Hiding

*TODO betterify definition*
_Latency hiding_ is a fancy term for something you do all the
time. You're hiding latency whenever you move a task that's in a
waiting state to the background and focus on something else. 
Examples abound, not just in programming but in real life.

If you use Clojure's +future+ function to kick off a task in a
separate thread so that the main task can continue unimpeded, you're
hiding latency. I've used +future+ on web servers to send an email
without increasing the overall response time for a user's request.

Latency hiding is often a cheap and easy way to get quick performance
gains. On the other hand, forgetting to employ it can lead to some
dire consequences, as this comic illustrates:

*TODO image*

You probably already use latency hiding all the time, even if you
don't call it that. Though you may be an old hand at it, I think it's
useful to have a name for it and to place it within the larger
performance context.

==== Functional Decomposition

_Functional decomposition_ is the term for when a multicultural group
of teenagers combine their powers to summon an avatar of the earth to
fight pollution:

*TODO image*

*TODO not just different threads. Different servers. Different
spaces/processes.*

*TODO already used this trope*

_Cough_ uh, I mean, _functional decomposition_ is the practice of
running logically independent program modules in parallel on separate
threads. As it turns out, all Java programs (including Clojure
programs) already do this: every Java program has a garbage collector
running on a separate thread.

Another common example functional decomposition is putting
long-running tasks on a queue so that a background thread can process
them without impeding the main thread. One of my site projects does
this: the main thread runs a web server, and the background thread
(launched with +future+) constantly works through a queue of RSS
feeds, checking for updates and putting the results in a database.

Functional decomposition will only give you a constant factor
speedup. When you split your code base into two modules and run them
on separate threads, you don't get any additional benefits if you
increase the number of cores on your machine.

If you squint a little bit, this strategy looks a lot like something
you do all the time on a larger scale. You run your web server and
database on separate machines. On a single machine, you run logically
independent modules as separate processes, also known as
programs. Like latency hiding, functional decomposition might be
something you're familiar with; it's just fun to know words for things
and their place in the greater order of the cosmos.

In the next section, I'm going to start venturing into unfamiliar
territory. Best grab your mosquito repellant and machete.

==== Data Parallelism

With the _data parallelism_ strategy, you divide a task into sub-tasks
that that don't have side effects and that can be executed
simultaneously. A dramatic example of this is seen in the _Terminator_
movies, where the task "destroy humankind" is divided into subtasks of
"destroy the humans you encounter" and executed by hunky killing
machines with Austrian accents. You can think of it as a kind of
parallelized reduce operation.

*TODO terminator image*

Another oft-used example is the +map+ function. On an abstract level,
+map+ derives a new collection from an existing one by applying a
function to every element of the original collection. There's nothing
in +map+'s semantics that requires the function applications to happen
in any particular order, and there's no shared state, so it's a
perfect candidate for parallelization. (In the literature, this kind
of easily-parallelized operation is called _embarrassing parallelism_,
which absolutely tickles me. "Pardon me! It appears that I've been
parallelized yet again, and in public no less!")

One hallmark of data parallelism is that it's _scalable_. If you
increase the amount of the work that needs to get done, it's possible
to reduce the amount of time needed to do it by throwing more
hardware at it. In the Terminator example, Skynet can eleminate mankind more
quickly by producing more terminators. In the map example, you can
complete the mapping more quickly by adding more cores.

Part of the fun in learning about data parallelism is discovering how
it can be used in cases beyond a simple map. The _scan_ operation, for
example, has a data dependency not present with map. Let's look at how
scan should work, then give it a definition and unpack how it differs
from map. There's no +scan+ function in Clojure, but here's how it
should behave:

[[scan behavior]]
[source,clojure]
---
(scan [1 2 3 4])
; => (1 3 6 10)

(scan [1 0 0])
; => (1 1 1)
---

Scan works by "rolling up" values as it traverses a sequence such
that each element of the resulting sequence is derived from previous
elements in the sequence. Let's call the initial sequence _x_ and the
result _y_. In the first example, _x~1~_ and _y~1~_ are
identical. _y~2~_ is the result of adding _x~1~_ and _x~2~_. _y~3~_ is
_x~1~_ \+ _x~2~_ \+ _x~3~_, and so on. You can see why scan is also
known as "cumulative sum" - each element of the result is sum of all
elements from the original sequence, up to that point.

The reason why it's not obvious how to parallelize this is that each
function application depends on the result of the previous
application. This becomes obvious when you look at a naive
implementation:

[[naive scan implementation]]
[source,clojure]
---
(defn scan
  [[x y & xs]]
  (if y
    (cons x (scan (cons (+ x y) xs)))
    (list x)))
---

This implementation is completely serial, with no hope of running in
parallel. Don't worry, though - you're going to learn how to
accomplish this parallelization.

*TODO explain why there's no hope*.
*TODO "in a concurrent universe" is a little weak*

Now that you understand how data parallelism compares to the other
main strategies for achieving performance in a concurrenct universe,
let's really dig into it so that you can understand it completely.

== Data Parallelism

At this point you probably have a vague intuition about how you might
be able to speed up your programs by using data parallelism. In this
section, you're going to refine your understanding by learning about
the _work-span model_, the premiere theoretical model for
understanding and predicting performance. You're also going to learn
the concrete implementation concerns you'll encounter when trying to
write parallel code.

=== Work-Span Model

The work-span model is concerned with two aspects of a parallel
algorithm: its _work_ and its _span_. _Work_ is the total number of
tasks that need to be completed, and _span_ is the length of the
longest path of work that has to be done serially. Take a look at this
diagram:

In the example on the left, you the work is 9 because there are 9
tasks that need to be completed, and the span is 5 because there are 5
tasks that have to be performed serially.

The span describes the upper limit to the amount of speedup you can
expect; past that, no amount of additional hardware will help your
algorithm run faster. 

== References

* Systems Performance: Enterprise and the Cloud
* Structured Parallel Programming
* Ebook on queueing systems

== TODO
* Explain how it differs from laziness
* No intermediate collections
* Talk about performance first?
* TODO lookup where I got my definition of efficiency as how well it
  makes use of computing resources / soakt he cores
* TODO mention that existing software needs to be able to run faster
  on new hardware
* TODO mention that reader should read the first section of concurrency chapter
